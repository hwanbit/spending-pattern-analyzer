import os
import glob
import random
import joblib
import numpy as np
import pandas as pd
from flask import Flask, request, jsonify
from flask_cors import CORS
from sklearn.cluster import MiniBatchKMeans
from llama_cpp import Llama

app = Flask(__name__)
CORS(app) # ëª¨ë“  ë„ë©”ì¸ í—ˆìš© (ê°œë°œìš©)

category_map = {}
major_categories = []
models = {}

# Kmeans ëª¨ë¸ ì €ì¥ ê²½ë¡œ í™•ë³´
model_dir = './model'
if not os.path.exists(model_dir):
    os.makedirs(model_dir)
    print(f"ğŸ“ '{model_dir}' í´ë”ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

# ==========================================
# 1. ë°ì´í„° ë¡œë“œ ë° ê³„ì¸µ êµ¬ì¡°(Hierarchy) êµ¬ì¶•
# ==========================================
def load_and_train_efficiently():
    global category_map, major_categories, models

    # ê²½ë¡œ ì„¤ì •
    base_dir = os.path.dirname(os.path.abspath(__file__))
    file_pattern = os.path.join(base_dir, '../data', 'tbsh_gyeonggi_day_*.csv')

    if not glob.glob(file_pattern):
        file_pattern = './data/tbsh_gyeonggi_day_*.csv'

    all_files = glob.glob(file_pattern)
    print(f"ğŸ” ë°œê²¬ëœ íŒŒì¼: {len(all_files)}ê°œ")

    if not all_files:
        return None, [], {}

    # ì¤‘ë¶„ë¥˜(nm_2)ê¹Œì§€ ì½ì–´ì˜´
    eng_cols = ['age', 'sex', 'card_tpbuz_nm_1', 'card_tpbuz_nm_2', 'amt']
    rename_map = {
        'age': 'ì—°ë ¹ë³„', 'sex': 'ì„±ë³„',
        'card_tpbuz_nm_1': 'ëŒ€ë¶„ë¥˜', 'card_tpbuz_nm_2': 'ì¤‘ë¶„ë¥˜',
        'amt': 'ë§¤ì¶œê¸ˆì•¡'
    }

    structure_data = set()
    aggregated_chunks = []
    local_category_map = {}

    print("ğŸš€ ë°ì´í„° ë¡œë”© ì‹œì‘...")

    for f in all_files:
        try:
            # ì¸ì½”ë”© ê°ì§€
            target_encoding = 'utf-8'
            try:
                # nrows=10ì„ ì¤˜ì„œ ì‹¤ì œ ë°ì´í„° 10ì¤„ì„ utf-8ë¡œ ì½ì–´ë´„
                # ë°ì´í„°ì— í•œê¸€ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ì—ëŸ¬ê°€ ë°œìƒí•˜ì—¬ exceptë¡œ ë„˜ì–´ê°
                pd.read_csv(f, usecols=eng_cols, nrows=10, encoding='utf-8')
            except UnicodeDecodeError:
                target_encoding = 'cp949' # utf-8 ì‹¤íŒ¨ ì‹œ cp949 í™•ì •

            print(f"   -> ì¸ì½”ë”© í™•ì •: {target_encoding} ({os.path.basename(f)})")

            # í™•ì •ëœ ì¸ì½”ë”©ìœ¼ë¡œ Chunk Iterator ìƒì„±
            chunk_iter = pd.read_csv(f, usecols=eng_cols, chunksize=100000, encoding=target_encoding)

            # ë°ì´í„° ì²˜ë¦¬
            for chunk in chunk_iter:
                chunk.rename(columns=rename_map, inplace=True)

                # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ìˆ˜ì§‘
                pairs = chunk[['ëŒ€ë¶„ë¥˜', 'ì¤‘ë¶„ë¥˜']].drop_duplicates()
                for _, row in pairs.iterrows():
                    structure_data.add((row['ëŒ€ë¶„ë¥˜'], row['ì¤‘ë¶„ë¥˜']))

                # í•™ìŠµ ë°ì´í„° ì§‘ê³„
                grouped = chunk.groupby(['ì—°ë ¹ë³„', 'ì„±ë³„', 'ì¤‘ë¶„ë¥˜'])['ë§¤ì¶œê¸ˆì•¡'].sum().reset_index()
                aggregated_chunks.append(grouped)

        except Exception as e:
            print(f"ğŸš« íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ({f}): {e}")
            continue

    if not aggregated_chunks:
        return None, [], {}

    # ì¹´í…Œê³ ë¦¬ ë§µ êµ¬ì¶•
    for major, middle in structure_data:
        if major not in local_category_map:
            local_category_map[major] = []

        local_category_map[major].append(middle)

    # ì •ë ¬
    for k in local_category_map: local_category_map[k].sort()

    # ë°ì´í„° ë³‘í•©
    total_df = pd.concat(aggregated_chunks, axis=0)
    final_df = total_df.groupby(['ì—°ë ¹ë³„', 'ì„±ë³„', 'ì¤‘ë¶„ë¥˜'])['ë§¤ì¶œê¸ˆì•¡'].sum().reset_index()

    features_list = sorted(final_df['ì¤‘ë¶„ë¥˜'].unique().tolist())

    # í”¼ë²— ë° ì •ê·œí™”
    pivot_df = final_df.pivot_table(index=['ì—°ë ¹ë³„', 'ì„±ë³„'], columns='ì¤‘ë¶„ë¥˜', values='ë§¤ì¶œê¸ˆì•¡', fill_value=0)
    model_data = pivot_df.div(pivot_df.sum(axis=1), axis=0).fillna(0)

    category_map = local_category_map
    major_categories = features_list

    return model_data, features_list, local_category_map

# ==========================================
# 2. ë‹¤ì¤‘ K ëª¨ë¸ ê´€ë¦¬ì
# ==========================================
class LargeScaleClusterManager:
    def __init__(self):
        self.models = {}
        self.k_levels = [i for i in range(3, 9)]
        self.feature_names = []
        self.model_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'model')

    def train(self):
        global major_categories, category_map

        # ì´ë¯¸ í•™ìŠµëœ .pkl íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
        feature_path = os.path.join(self.model_dir, 'feature_names.pkl')
        map_path = os.path.join(self.model_dir, 'category_map.pkl')

        # ì €ì¥ëœ ë°ì´í„°(ì»¬ëŸ¼, ë§µ) ë¡œë“œ ì‹œë„
        is_data_ready = False
        if os.path.exists(feature_path) and os.path.exists(map_path):
            try:
                self.feature_names = joblib.load(feature_path)
                loaded_map = joblib.load(map_path)

                # ì „ì—­ ë³€ìˆ˜ ë™ê¸°í™”
                major_categories = self.feature_names
                category_map.update(loaded_map) # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸

                print(f"âœ… ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ì»¬ëŸ¼: {len(major_categories)}ê°œ)")
                is_data_ready = True
            except:
                print("âš ï¸ ë°ì´í„° íŒŒì¼ ì†ìƒ, ë‹¤ì‹œ ë§Œë“­ë‹ˆë‹¤.")

        # ëª¨ë¸ íŒŒì¼ ë¡œë“œ ì‹œë„
        all_models_exist = True
        for k in self.k_levels:
            path = os.path.join(self.model_dir, f'kmeans_k{k}.pkl')
            if os.path.exists(path):
                try:
                    self.models[k] = joblib.load(path)
                except:
                    all_models_exist = False
            else:
                all_models_exist = False

        # ëª¨ë¸ì´ë‚˜ ì»¬ëŸ¼ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì „ì²´ ì¬í•™ìŠµ
        if not all_models_exist or not self.feature_names:
            print("âš ï¸ ëª¨ë¸ ë˜ëŠ” ë°ì´í„°ê°€ ì—†ì–´ ìƒˆë¡œ í•™ìŠµí•©ë‹ˆë‹¤...")
            data, features, cat_map = load_and_train_efficiently()

            if data is None:
                print("âŒ í•™ìŠµ ì‹¤íŒ¨: ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

            self.feature_names = features
            major_categories = features
            category_map.update(cat_map)

            joblib.dump(features, feature_path)
            joblib.dump(cat_map, map_path)
            print(f"ğŸ’¾ ì»¬ëŸ¼ ì •ë³´ ì €ì¥ ì™„ë£Œ: {feature_path}")

            print("ğŸš€ ëª¨ë¸ í•™ìŠµ ì‹œì‘ (MiniBatchKMeans)...")
            for k in self.k_levels:
                # MiniBatchKMeansëŠ” ì†ë„ê°€ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ë¥¼ ì ê²Œ ì”€
                kmeans = MiniBatchKMeans(
                    n_clusters=k,
                    batch_size=2048,  # í•œ ë²ˆì— í•™ìŠµí•  ìƒ˜í”Œ ìˆ˜
                    random_state=42,
                    n_init=10
                )
                kmeans.fit(data)
                self.models[k] = kmeans

                # ëª¨ë¸ ì €ì¥ (ì„œë²„ ì¬ì‹œì‘ ì‹œ ì €ì¥ëœ ëª¨ë¸ ì‚¬ìš©)
                joblib.dump(kmeans, f'./model/kmeans_k{k}.pkl')

            print("ğŸ‰ ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")

    def get_feature_names(self):
        if self.feature_names:
            return self.feature_names
        else:
            # ë©”ëª¨ë¦¬ì— ì—†ìœ¼ë©´ íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
            feature_path = os.path.join(self.model_dir, 'feature_names.pkl')
            if os.path.exists(feature_path):
                self.feature_names = joblib.load(feature_path)
                return self.feature_names
            return [] # í•™ìŠµëœ ê²Œ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    def predict(self, user_vector_norm, k):
        model = self.models.get(k)
        if not model:
            return None, None

        # Feature ê°œìˆ˜ ê²€ì¦
        if user_vector_norm.shape[1] != model.n_features_in_:
            print(f"âš ï¸ ì°¨ì› ë¶ˆì¼ì¹˜: ëª¨ë¸({model.n_features_in_}) vs ì…ë ¥({user_vector_norm.shape[1]})")
            # ì—ëŸ¬ ë°©ì§€, ê°•ì œë¡œ ë§ì¶¤
            if user_vector_norm.shape[1] < model.n_features_in_:
                user_vector_norm = np.pad(user_vector_norm, ((0, 0), (0, model.n_features_in_ - user_vector_norm.shape[1])))
            else:
                user_vector_norm = user_vector_norm[:, :model.n_features_in_]

        cid = model.predict(user_vector_norm)[0]
        centroid = model.cluster_centers_[cid]

        return cid, centroid, user_vector_norm

# ì‹¤í–‰ë¶€ ìˆ˜ì •
cluster_manager = LargeScaleClusterManager()
# ì„œë²„ ì‹œì‘ ì‹œ í•™ìŠµ ì‹¤í–‰ (ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë“œí•˜ê³  íŒ¨ìŠ¤)
cluster_manager.train()
feature_names = cluster_manager.feature_names

# ==========================================
# 3. ë¡œì»¬ LLM ë˜í¼
# ==========================================
# ì‹¤ì œ GGUF íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”
MODEL_PATH = "./model/EXAONE-3.5-2.4B-Instruct-Q4_K_M.gguf"

try:
    llm = Llama(model_path=MODEL_PATH, n_ctx=2048, verbose=False)
    has_llm = True
except:
    print("Warning: GGUF ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Mock ì‘ë‹µì„ ë³´ëƒ…ë‹ˆë‹¤.")
    has_llm = False

def generate_persona(features_desc, max_retries=2):
    # ì†Œë¹„ íŒ¨í„´ ê¸°ë°˜ ë‹‰ë„¤ì„ê³¼ í•´ì‹œíƒœê·¸ ìƒì„±
    if not has_llm:
        return "ë¶„ì„ ì™„ë£Œ (AI ë¯¸ì—°ë™)", "#ë°ì´í„° #ë¶„ì„ #ì¤€ë¹„ì¤‘"

    # Few-shot í”„ë¡¬í”„íŠ¸
    prompt = f"""ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ë‹‰ë„¤ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì†Œë¹„ íŒ¨í„´ì„ ë³´ê³  ì¬ë¯¸ìˆëŠ” ë‹‰ë„¤ì„ê³¼ í•´ì‹œíƒœê·¸ë¥¼ ë§Œë“œì„¸ìš”.

ì˜ˆì‹œ 1:
ì†Œë¹„ íŒ¨í„´: ì¹´í˜ 65%, ë² ì´ì»¤ë¦¬ 25%, ë””ì €íŠ¸ 10%
ë‹‰ë„¤ì„ | #íƒœê·¸1 #íƒœê·¸2 #íƒœê·¸3
ì¹´í˜ë§ˆë²•ì‚¬ | #ì»¤í”¼í™€ë¦­ #ë””ì €íŠ¸ì™• #ì¹´í˜ìˆœë¡€

ì˜ˆì‹œ 2:
ì†Œë¹„ íŒ¨í„´: ì˜¨ë¼ì¸ì‡¼í•‘ 70%, ë°°ì†¡ë¹„ 20%, ë°˜í’ˆ 10%
ë‹‰ë„¤ì„ | #íƒœê·¸1 #íƒœê·¸2 #íƒœê·¸3
íƒë°°ê¸°ë‹¤ë¦¬ë¯¸ | #ì‡¼í•‘ì™• #í´ë¦­ì¤‘ë… #ë°°ì†¡ì¶”ì 

ì˜ˆì‹œ 3:
ì†Œë¹„ íŒ¨í„´: í¸ì˜ì  50%, ì•¼ì‹ 30%, íƒì‹œ 20%
ë‹‰ë„¤ì„ | #íƒœê·¸1 #íƒœê·¸2 #íƒœê·¸3
ì•¼í–‰ì„±ì¸ê°„ | #í¸ì˜ì ëŸ¬ë²„ #ì•¼ì‹íŒŒí‹° #íƒì‹œì™•

ì´ì œ ì•„ë˜ íŒ¨í„´ìœ¼ë¡œ ì°½ì˜ì ìœ¼ë¡œ ë§Œë“œì„¸ìš”:
ì†Œë¹„ íŒ¨í„´: {features_desc}
ë‹‰ë„¤ì„ | #íƒœê·¸1 #íƒœê·¸2 #íƒœê·¸3
"""

    for attempt in range(max_retries + 1):
        try:
            output = llm(
                prompt,
                max_tokens=60,
                temperature=0.8 + (attempt * 0.1),  # ì¬ì‹œë„ë§ˆë‹¤ ì°½ì˜ì„± ì¦ê°€ë¡œ ë™ì¼ ë‹‰ë„¤ì„ì´ ìƒì„±ë˜ì§€ ì•Šë„ë¡ í•¨
                top_p=0.9,
                top_k=40,
                repeat_penalty=1.2,  # ë°˜ë³µ ì–µì œ
                stop=["\n", "\n\n", "ì˜ˆì‹œ"],
                echo=False
            )
            result = output['choices'][0]['text'].strip()

            # ë‹‰ë„¤ì„ì— ë”°ì˜´í‘œì™€ ë³„í‘œ ì œê±°
            result = result.replace('"', '').replace("'", '').replace('"', '').replace('"', '')
            result = result.replace('*', '')

            # íŒŒì‹±
            if "|" in result:
                parts = result.split("|", 1)
                nickname = parts[0].strip()
                tags = parts[1].strip()

                # ìœ íš¨ì„± ê²€ì‚¬ ê°•í™”
                if (nickname and tags and
                    tags.startswith("#") and
                    len(nickname) >= 2 and
                    nickname not in ["ë‹‰ë„¤ì„", "ì†Œë¹„ì™•", "ë¶„ì„ì¤‘, ë‹¤ì‹œ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."] and
                    "#ë¶„ì„ì¤‘" not in tags):
                    return nickname, tags

            # ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„
            if attempt < max_retries:
                print(f"[Debug] ì¬ì‹œë„ {attempt+1}/{max_retries}: {result}")
                continue

        except Exception as e:
            print(f"[Error] LLM ì˜¤ë¥˜ (ì‹œë„ {attempt+1}): {e}")
            if attempt < max_retries:
                continue

    # ìµœì¢… ì‹¤íŒ¨ ì‹œ íŒ¨í„´ ê¸°ë°˜ ê¸°ë³¸ê°’
    fallback_nicknames = ["ì†Œë¹„íƒí—˜ê°€", "ì§€ê°‘ì§€í‚´ì´", "ì•Œëœ°ì™•", "ì”€ì”€ì´ë§ˆìŠ¤í„°"]
    return random.choice(fallback_nicknames), "#ì†Œë¹„íŒ¨í„´ #ë°ì´í„°ë¶„ì„ #ë¼ì´í”„ìŠ¤íƒ€ì¼"

# ==========================================
# 4. Flask ë¼ìš°íŒ… ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
# ==========================================
@app.route('/categories', methods=['GET'])
def get_categories():
    # ë°ì´í„° ë§¤ë‹ˆì €ì— ì¹´í…Œê³ ë¦¬ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜
    if not category_map:
            return jsonify({"Error": ["ë°ì´í„° ë¡œë”© ì‹¤íŒ¨"]}), 500
    return jsonify(category_map)

@app.route('/analyze', methods=['POST'])
def analyze():
    # ëª¨ë¸ ì¤€ë¹„ í™•ì¸
    if not feature_names:
        return jsonify({"error": "Server is initializing..."}), 503

    req = request.json
    k = int(req.get('k', 3))
    items = req.get('items', []) # items ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°

    # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì˜¨ items ë¦¬ìŠ¤íŠ¸ë¥¼ {ì¹´í…Œê³ ë¦¬: ê¸ˆì•¡} í˜•íƒœë¡œ 1ì°¨ ê°€ê³µ
    user_sums = {}
    for item in items:
        middle = item.get('middle')
        amount = float(item.get('amount', 0))
        user_sums[middle] = user_sums.get(middle, 0) + amount

    # ë²¡í„° ìƒì„± (ëª¨ë¸ ê¸°ì¤€ ìˆœì„œ feature_namesë¥¼ ë”°ë¦„)
    user_vector = []
    total = 0

    matched_log = []

    for feat in feature_names:
        # í•©ì‚°í•œ user_sumsì—ì„œ ê°’ì„ ì°¾ìŒ
        val = user_sums.get(feat, 0)

        if val > 0:
            matched_log.append(f"{feat}: {val}")

        user_vector.append(val)
        total += val

    if total == 0:
        print("âŒ ë§¤ì¹­ ì‹¤íŒ¨: ì…ë ¥í•œ ì¹´í…Œê³ ë¦¬ ì´ë¦„ì´ ë°±ì—”ë“œ feature_namesì™€ ë‹¤ë¦…ë‹ˆë‹¤.")
        print(f"   (íŒíŠ¸) ë°±ì—”ë“œê°€ ê¸°ëŒ€í•˜ëŠ” ì´ë¦„ ì˜ˆì‹œ: {feature_names[:5]}...")
        return jsonify({
            "error": "ë°ì´í„° ë§¤ì¹­ ì‹¤íŒ¨. ì…ë ¥í•œ ì¹´í…Œê³ ë¦¬ê°€ í•™ìŠµ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.",
            "debug_info": {
                "input_keys": list(user_sums.keys()),
                "expected_sample": feature_names[:5]
            }
        }), 400

    # ì •ê·œí™”
    user_vector_norm = np.array([v/total for v in user_vector]).reshape(1, -1)

    # í´ëŸ¬ìŠ¤í„° ì˜ˆì¸¡
    cluster_id, centroid, final_vec = cluster_manager.predict(user_vector_norm, k)
    if cluster_id is None:
        return jsonify({"error": "Model Error"}), 500

    # ì¹´í…Œê³ ë¦¬ ì´ë¦„ í™•ë³´
    cats = feature_names
    my_vec = user_vector_norm[0]

    # Gap Analysis - ë‚˜ì™€ í‰ê· ì˜ ì°¨ì´ ë¶„ì„
    # (ë‚´ ë¹„ìœ¨ - ê·¸ë£¹ í‰ê·  ë¹„ìœ¨)
    diff = user_vector_norm[0] - centroid
    group_max_idx = np.argmax(centroid)
    group_main_cat = cats[group_max_idx] if group_max_idx < len(cats) else "ê¸°íƒ€"

    # ë‚˜ì˜ íŠ¹ì§• ì°¾ê¸°
    my_max_idx = np.argmax(user_vector_norm[0])
    my_max_val = user_vector_norm[0][my_max_idx]
    my_main_cat = cats[my_max_idx] if my_max_idx < len(cats) else "ê¸°íƒ€"

    # ë‚´ê°€ ê·¸ë£¹ë³´ë‹¤ ì••ë„ì ìœ¼ë¡œ ë§ì´ ì“°ëŠ” ê²ƒ (Gap Max)
    gap_max_idx = np.argmax(diff)
    gap_max_cat = cats[gap_max_idx]

    # ë‚´ê°€ ê·¸ë£¹ë³´ë‹¤ í›¨ì”¬ ì ê²Œ ì“°ëŠ” ê²ƒ (Gap Min)
    gap_min_idx = np.argmin(diff)
    gap_min_cat = cats[gap_min_idx]

    feature_desc = ""
    prompt_style = "normal" # normal | obsessed

    # ì¡°ê±´ë¶€ í”„ë¡¬í”„íŠ¸ ìƒì„±
    if my_max_val >= 0.9: # í•œ ì¹´í…Œê³ ë¦¬ì— 90% ì´ìƒ ì¼ë‹¤ë©´?
        feature_desc = f"Obsessed with {my_main_cat} (Spending {int(my_max_val*100)}% of money only on {my_main_cat})"
        prompt_style = "obsessed"
    else:
        prompt_style = "normal"
        top_indicies = my_vec.argsort()[-2:][::-1]

    if prompt_style == "obsessed":
         # í•œ ìš°ë¬¼ íŒŒëŠ” ê²½ìš° -> "ì¥ì¸", "ë§¤ë‹ˆì•„" ê°™ì€ ë‹¨ì–´ ìœ ë„
        llm_input = f"{feature_desc}. This person loves {my_main_cat} too much."
    else:
        llm_input = feature_desc

        # 0ë³´ë‹¤ í° í•­ëª©ë§Œ ì„¤ëª…ì— í¬í•¨
        desc = []
        for i in top_indicies:
            if my_vec[i] > 0.01: # 1% ì´ìƒ ì“´ ê²ƒë§Œ
                desc.append(f"{cats[i]}(ë§ìŒ)")

        if not desc:
            llm_input = "Characteristic: Normal with no particular place to spend money."
        else:
            llm_input = "money spending pattern: " + ", ".join(desc)

    print(f"ğŸ“ LLM ìš”ì²­: {llm_input}")
    nickname, tags = generate_persona(llm_input)

    # ë§Œì•½ LLMì´ ë¹ˆ ê°’ì„ ë±‰ìœ¼ë©´, ë£°ë² ì´ìŠ¤ ë°±ì—… ë‹‰ë„¤ì„ ì œê³µ
    if not nickname or "Explanation" in nickname:
        if prompt_style == "obsessed":
            nickname = f"{my_main_cat} ë§¤ë‹ˆì•„"
            tags = f"#{my_main_cat} #{group_main_cat} #ë§ˆìŠ¤í„°"
        else:
            nickname = "í•©ë¦¬ì ì¸ ë°¸ëŸ°ìŠ¤ì¡±"
            tags = f"#{my_main_cat} #{group_main_cat} #ìŠ¤ë§ˆíŠ¸ ì»¨ìŠˆë¨¸"

    # Case A: ë‚´ê°€ ê·¸ë£¹ë³´ë‹¤ ì ê²Œ ì“°ëŠ” ë¶€ë¶„ (Saving)
    if gap_min_cat == group_main_cat:
        # ê·¸ë£¹ì€ ì´ê±¸ ì¢‹ì•„í•˜ëŠ”ë° ë‚˜ëŠ” ì•ˆ ì“°ëŠ” ê²½ìš°
        saving_text = f"ì´ ì†Œë¹„ ê·¸ë£¹ì€ '{group_main_cat}' ì†Œë¹„ê°€ í•µì‹¬ì¸ë°, ì‚¬ìš©ìë‹˜ì€ ì´ ë¶€ë¶„ì—ì„œ ëˆì„ ì•„ë¼ì…¨ì–´ìš”!"
    else:
        saving_text = f"ë³´í†µ '{group_main_cat}'ì„ ë§ì´ ì†Œë¹„í•˜ëŠ” ê·¸ë£¹ì´ì§€ë§Œ, ì‚¬ìš©ìë‹˜ì€ ì´ ì†Œë¹„ ê·¸ë£¹ë³´ë‹¤ '{gap_min_cat}' ì†Œë¹„ê°€ ì ì€ í¸ì´ì—ìš”."

    # Case B: ë‚´ê°€ ê·¸ë£¹ë³´ë‹¤ ë§ì´ ì“°ëŠ” ë¶€ë¶„ (Unique)
    if my_main_cat == group_main_cat:
        unique_text = f"ì´ ì†Œë¹„ ê·¸ë£¹ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì‚¬ìš©ìë‹˜ë„ '{my_main_cat}'ì— ì§„ì‹¬ì´ì‹œêµ°ìš”! (í‰ê· ë³´ë‹¤ {int(diff[gap_max_idx] * 100)}%p ë” ë†’ìŒ)"
    else:
        unique_text = f"ëŒ€ì²´ë¡œ '{group_main_cat}' ìœ„ì£¼ ì†Œë¹„ ê·¸ë£¹ì— ì†í•˜ì‹œì§€ë§Œ, ì‚¬ìš©ìë‹˜ì€ '{my_main_cat}' ì·¨í–¥ì´ í™•ê³ í•˜ì‹œë„¤ìš”!"

    # Kê°’ ë³€í™”ì— ë”°ë¥¸ íˆìŠ¤í† ë¦¬ (Lineage)
    # ì‚¬ìš©ìê°€ í˜„ì¬ ì„ íƒí•œ k ì™¸ì— ë‹¤ë¥¸ kì—ì„œëŠ” ì–´ë””ì— ì†í•˜ëŠ”ì§€ ê³„ì‚°í•´ì„œ ë°˜í™˜
    lineage = {}
    for k in cluster_manager.k_levels:
        cid, _, _ = cluster_manager.predict(user_vector_norm, k)
        lineage[f"k={k}"] = int(cid)

    return jsonify({
        "current_k": k,
        "cluster_id": int(cluster_id),
        "persona_nickname": nickname,
        "persona_tags": tags,
        "gap_analysis": {
                "unique_trait": unique_text,
                "saving_trait": saving_text
            },
        "cluster_lineage": lineage,
        "debug_vector": [user_vector_norm[0].tolist()],
        "group_vector": centroid.tolist()
    })

if __name__ == '__main__':
    app.run(debug=True, port=5000)